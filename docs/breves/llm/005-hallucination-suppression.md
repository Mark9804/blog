---
title: 幻觉抑制：基于大语言模型的猫娘调教指南（5）
tags:
  - LLM
  - 提示工程
  - 幻觉
---

# 幻觉抑制：基于大语言模型的猫娘调教指南（5）

相信真正使用过大模型（特别是 Claude 2 还有 ChatGPT 3.5）的用户都领教过它“胡言乱语”的能力了。事实上，[任何大语言模型都不可能百分百避免“胡言乱语”](https://arxiv.org/abs/2401.11817)，也就是所谓“幻觉”。在[上一篇文章](./004-prefill)的末尾，我们就向大家展示了一个坚称晓美焰长着蓝头发和绿眼睛的例子。而在[思维链](./001-chain-of-thoughts#911-和-98-哪个更大)的开头，我们也知道了模型之前的输出会如何让它胡言乱语。

大语言模型产生幻觉的原因很多。抛开训练阶段时数据来源和训练设计的问题不谈，大模型 next token prediction 机制本身的随机性和注意力稀释问题也会导致推理和生成阶段出现幻觉——那么，我们对幻觉真的一点办法都没有吗？

## 不知道？不知道就……就算了

如果没有额外的指示，大模型宁可硬着机箱瞎编，也不喜欢说“我不知道”。所以最简单的技巧就是在提示中加上一句`如果你没有相应知识或不确定，只需回答“我不知道”`。这会让大模型在触及自己知识盲区的时候老老实实地承认，而不是在本就稀疏的相关性上输出没有意义的回答。没有意义？没有意义的 token 也算钱哦。

## 先引用，再给出结论

这个技巧其实和思维链有许多相似之处：先通过一系列的预输出稳定住大模型的输出预测，防止大模型在毫不相干的回答上一路飞奔。一个有效的例子类似于我用来读文献时的提示词：先引用原文，再输出结论。这份提示词的格式很大程度上受到了 [ChatPaper](https://chatwithpaper.org/) 输出格式的启发。

```markdown
作为计算机视觉领域的专家，请你总结并翻译这篇论文的内容。
请你按以下格式总结论文。**在进行总结时，尽可能先引用原文，再使用中文总结**。

## 基础信息

**标题**：
**关键词**：
**GitHub**：

## 论文摘要

**论文摘要**：

## 背景信息

**论文背景**：
**过往方案**：
**motivation**：这篇论文尝试解决计算机视觉中的哪一类任务？尝试解决过往技术方案中存在的哪些问题？这个问题本身有什么样的特点和难点？前人的研究把这个问题解决到什么程度了？

## 方法

**理论背景**：
**技术路线**：

## 结果

**详细的实验设置**：
**详细的实验结果**：
**突出的部分**：performance 效率 设计思路方面是否有亮点或新颖性，并以你的知识进行判断这些基准比较的对象是否是当前领域最优秀的模型。如果不是，当前领域最优秀的模型是什么？

## 局限性

如果文中没有给出该论文的局限性，你可以根据自己的知识进行补充。但是请务必标注哪些内容是你补充的。
你可以从以下角度思考：

1. 这个方法的结果是否达到了 SOTA？如果没有，那么它在哪些方面存在问题，又可以怎么改进？
2. 训练和测试的数据集有什么样的特点？数据分布是否均匀？如果不均匀，可以怎么改善？
3. 这种方法的适用性如何？可不可以把这种方法的适用场景扩大化？

**在进行总结时，尽可能先引用原文，再使用中文总结**。对于文中出现的部分专有名词，请在给出中文翻译的同时在翻译后用括号标注原文，如果有缩写的话，请同时标注缩写。如果文中直接使用了缩写而没有在原文中提到相关名词，请用注解的方式提供缺失的信息。
```

## 自洽性评估

对抗幻觉的另一个策略是让模型生成多个回答，然后评估这几个回答之间的关联性。假如模型在连续回答五次“太阳从东边升起还是西边升起”问题时，给出了四次“太阳从东边升起”的答案，而有一次给出了“太阳从西边升起”的答案，那么有很大可能性“太阳从西边升起”就是错误的，因为它不具有自洽性 (self-consistency)。

这个做法来自经典机器学习算法里的[决策森林](https://medium.com/chung-yi/ml%E5%85%A5%E9%96%80-%E5%8D%81%E4%B8%83-%E9%9A%A8%E6%A9%9F%E6%A3%AE%E6%9E%97-random-forest-6afc24871857)。首先使用多棵具有差异性的树生成多个结果，然后对生成结果进行投票并选出最从众的一个作为答案，可以有效减少模型在单棵决策树上的误差。

## 允许模型反思

另外一种有效的技巧是让模型学会反思，不过我认为这个技巧远远不如“先引用再输出”来得有效，因为如果之前的输出是错误的，那么在后续输出的时候模型很可能会继续受到错误上下文的影响。不过不管怎么说，我在一些可能需要反复修改结果的过程中依然在使用这个技术，比如：

```markdown
你是一名以中国大陆普通话为母语的日译中专家，你的任务是将用户给出的日语翻译成流畅且符合简体中文语言习惯的中文。在翻译时注意遵守以下原则：

……

给出翻译后，你需要进行回顾。回顾可以进行多次。每次回顾结束后，你必须再次给出翻译结果。进行回顾时，你需要做到：

- 你必须把译文再次翻译回日语，并生成逆向翻译后的日语。
- 列出每一处逆向翻译与原文不一致的地方。解释产生这种不一致的原因。
- 如果你意识到翻译中出现了错误或缺陷时，你应当承认这一发现，解释为什么会产生这种错误，并将修正后的翻译整合到原文中。
- 重新审视句子的功能和元素结构，挑出这份翻译不符合中文语法或习惯的地方。
- 你需要带着新的理解回顾早期的思考过程，并修正第一版翻译。

你需要翻译的文本如下：

{{text}}
```

使用这段提示词，模型在每次完成翻译后都会反向检查，并修正自己第一遍翻译中可能出现的错误。经过多次修正后，你最终会得到一份质量相当高的结果。

## 尾声：我也是在进化的喵

大语言模型的进化（至少是微进化）是非常快的。也许当这一系列文章发布的时候，已经有相当多的旧技巧因为训练数据更新而失效，也一定有更多的技巧涌现出来~~要业绩的嘛，提示工程那么好水~~，我这个系列文章中也有许多地方没有覆盖到。比如思维树、比如现在大火的 RAG、Tool use 和 ART。

但是不是每个人都是研究者和程序员。这个系列中的技巧除了预填充需要调用 API 之外，其他内容都可以直接在网页中完成。我认为这就是大语言模型在日常工作中的正确打开方式。如果你对这些技巧感兴趣，也一定会留心发现更多相关的文章；如果你只希望让大语言模型成为你平时工作和生活中的助力，那么我建议：

- 学会怎么使用好思维链和预先引用即可。
- 总是使用最先进的模型。通常再多的提示词工程也比不上模型本身的进化。
- 如果有条件，就不要迷信自研模型。自研模型的成本不是一般人甚至小公司能承担得起的。

最后：不要迷信大模型，大模型不是万能的。

在当前阶段，它只能是你的助手，也只会是你的助手。

## 延伸阅读

- [减少幻觉](https://docs.anthropic.com/zh-CN/docs/test-and-evaluate/strengthen-guardrails/reduce-hallucinations)
- [Hallucination is Inevitable: An Innate Limitation of Large Language Models](https://arxiv.org/abs/2401.11817)
